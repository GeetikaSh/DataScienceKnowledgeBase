# Topics to Cover

- *Embeddings & Tokenization:*
    - WordPiece, BPE, SentencePiece
    - Positional embeddings (absolute vs. rotary)
- Language Model Types:
    - Encoder (BERT), Decoder (GPT), Encoder-Decoder (T5, BART)
- Loss Functions:
    - Masked LM Loss (BERT)
    - Causal LM Loss (GPT)
    - Seq2Seq loss for encoder-decoder models
- Pretraining Objectives:
    - MLM, CLM, DAE, span masking (T5), permutation LM (XLNet)
- Evaluation Metrics:
    - Perplexity, BLEU, ROUGE, METEOR, BERTScore